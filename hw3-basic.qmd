---
title: "hw3-basic"
author: "Anjani Sowmya Bollapragada - 23851219"
format: html
editor: visual
---

---
title: "Classification: Basic Concepts and Techniques"
author: "Anjani Sowmya Bollapragada - 23851219"
format: html
editor: visual
---

## Installing packages

```{r}
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(tidyverse, rpart, rpart.plot, caret, 
  lattice, FSelector, sampling, pROC, mlbench, rJava)
```

## Introduction

Classification is a machine learning task with the goal to learn a predictive function of the form y = f(x) where x is called the attribute set and y the class label. The attribute set consists of feature which describe an object. 

Classification learns the classification model from training data where both the features and the correct class label are available. This is why it is called a supervised learning problem.

A related supervised learning problem is regression, where y is a number instead of a label. Linear regression is a very popular supervised learning model, however, we will not talk about it here since it is taught in almost any introductory statistics course.

## The Artists Dataset

The Artists dataset containing 14 variables for many artists as a data frame with 14 columns (artist name,	edition number,	year,	artist nationality,	artist nationality (other),	artist gender,	artist race, artist ethnicity,	book,	space ratio per page total,	artist unique id,	moma count to year,	whitney count to year,	artist race (nwi)). 

```{r}
artists <- read_csv('artists.csv')

head(artists)
```
dropping missing values

```{r}
artists <- na.omit(artists)
```

translated all the TRUE/FALSE values into factors (nominal). This is often needed for building models.

```{r}
artists <- artists |>
  mutate(across(where(is.character), factor))
```

```{r}

summary(artists)
```

## Decision Trees

Recursive Partitioning (similar to CART) uses the Gini index to make splitting decisions and early stopping (pre-pruning).

```{r}
library(rpart)
```

Created Tree With Default Settings (uses pre-pruning)

```{r}
tree_default <- artists |> 
  rpart(book~ ., data = _)
tree_default
```

Plotting the graph

```{r}
library(rpart.plot)
rpart.plot(tree_default, extra = 2)
```

### Creating a Full Tree

To create a full tree, we should set the complexity parameter cp to 0

```{r}
tree_full <- artists |> 
  rpart(book ~ . , data = _, 
        control = rpart.control(minsplit = 2, cp = 0))
rpart.plot(tree_full, extra = 2, 
           roundint=FALSE,
            box.palette = list("Gy", "Gn", "Bu", "Bn", 
                               "Or", "Rd", "Pu")) # specify 7 colors
```

```{r}
tree_full
```

Training error on tree with pre-pruning

```{r}
predict(tree_default, artists) |> head ()
```

```{r}
pred <- predict(tree_default, artists, type="class")
head(pred)
```

```{r}
confusion_table <- with(artists, table(book, pred))
confusion_table
```

```{r}
correct <- confusion_table |> diag() |> sum()
correct
```

```{r}
error <- confusion_table |> sum() - correct
error
```

```{r}
accuracy <- correct / (correct + error)
accuracy
```

Using a function for accuracy

```{r}
accuracy <- function(truth, prediction) {
    tbl <- table(truth, prediction)
    sum(diag(tbl))/sum(tbl)
}

accuracy(artists |> pull(book), pred)
```

Training error of full tree

```{r}
accuracy(artists |> pull(book), 
         predict(tree_full, artists, type = "class"))
```

Getting a confusion table with more statistics (using caret)

```{r}
library(caret)
confusionMatrix(data = pred, 
                reference = artists |> pull(book))
```

### Make Predictions for New Data

Make up my own artist: A non hispatic latin origin German Americal artist (book: gardner)

```{r}
my_artist <- tibble(artist_name = "Aaron Douglas",	edition_number = 9,	year = 2023,	artist_nationality = "German-American",	artist_nationality_other = "Other",	artist_gender = "Male",	artist_race	= "White", artist_ethnicity = "Not Hispanic or Latino origin",	book = "Gardner",	space_ratio_per_page_total = 0.456128956,	artist_unique_id = 7,	moma_count_to_year = 2,	whitney_count_to_year	= 0, artist_race_nwi = "White")

```

Fixing columns to be factors like in the training set.

```{r}
my_artist <- my_artist |> 
  mutate(across(where(is.character), factor))
my_artist
```

Making a prediction using the default tree

```{r}
predict(tree_default , my_artist, type = "class")
```

## Model Evaluation with Caret

The package caret makes preparing training sets, building classification (and regression) models and evaluation easier. 

```{r}
library(caret)
```

Set random number generator seed to make results reproducible

```{r}
set.seed(2000)
```

### Hold out Test Data

Test data is not used in the model building process and set aside purely for testing the model. Here, we partition data the 80% training and 20% testing.

```{r}
inTrain <- createDataPartition(y = artists$book, p = .8, list = FALSE)
artists_train <- artists |> slice(inTrain)
```

```{r}
artists_test <- artists |> slice(-inTrain)
```

### Learn a Model and Tune Hyperparameters on the Training Data

The package caret combines training and validation for hyperparameter tuning into a single function called train(). It internally splits the data into training and validation sets and thus will provide with error estimates for different hyperparameter settings. trainControl is used to choose how testing is performed.

```{r}
fit <- artists_train |>
  train(book ~ .,
    data = _ ,
    method = "rpart",
    control = rpart.control(minsplit = 2),
    trControl = trainControl(method = "cv", number = 10),
    tuneLength = 5)

fit
```

A model using the best tuning parameters and using all the data supplied to train() is available as fit$finalModel.

```{r}
rpart.plot(fit$finalModel, extra = 2,
  box.palette = list("Gy", "Gn", "Bu", "Bn", "Or", "Rd", "Pu"))
```

caret also computes variable importance. By default it uses competing splits (splits which would be runners up, but do not get chosen by the tree) for rpart models 

```{r}
varImp(fit)
```

Here is the variable importance without competing splits.

```{r}
imp <- varImp(fit, compete = FALSE)
imp
```

```{r}
ggplot(imp)
```

## Testing: Confusion Matrix and Confidence Interval for Accuracy

Use the best model on the test data

```{r}
pred <- predict(fit, newdata = artists_test)
pred
```

Caretâ€™s confusionMatrix() function calculates accuracy, confidence intervals, kappa and many more evaluation metrics. 

```{r}
confusionMatrix(data = pred, 
                ref = artists_test |> pull(book))
```

## Model Comparison

Comparing decision trees with a k-nearest neighbors (kNN) classifier. Creating fixed sampling scheme (10-folds) to compare the different models using exactly the same folds. It is specified as trControl during training.

```{r}
train_index <- createFolds(artists_train$book, k = 10)
```

Build models

```{r}
rpartFit <- artists_train |> 
  train(book ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv", indexOut = train_index)
  )
```

```{r}
knnFit <- artists_train |> 
  train(book ~ .,
        data = _,
        method = "knn",
        preProcess = "scale",
          tuneLength = 10,
          trControl = trainControl(method = "cv", indexOut = train_index)
  )
```

Compare accuracy over all folds.

```{r}
resamps <- resamples(list(
        CART = rpartFit,
        kNearestNeighbors = knnFit
        ))

summary(resamps)
```

Caret provides some visualizations using the package lattice. For example, a boxplot to compare the accuracy and kappa distribution (over the 10 folds).

```{r}
library(lattice)
bwplot(resamps, layout = c(3, 1))
```

kNN is performing consistently better on the folds than CART (except for some outlier folds).

Finding out if one models is statistically better than the other (is the difference in accuracy is not zero).

```{r}
difs <- diff(resamps)
difs
```

```{r}
summary(difs)
```

p-values tells  the probability of seeing an even more extreme value (difference between accuracy) given that the null hypothesis (difference = 0) is true. 
For a better classifier, the p-value should be less than .05 or 0.01. diff automatically applies Bonferroni correction for multiple comparisons. In this case, kNN seems better but the classifiers do not perform statistically differently.

## Feature Selection and Feature Preparation

Decision trees implicitly select features for splitting, but we can also select features manually.

```{r}
library(FSelector)
```

### Univariate Feature Importance Score

These scores measure how related each feature is to the class variable. For discrete features (as in our case), the chi-square statistic can be used to derive a score.

```{r}
weights <- artists_train |> 
  chi.squared(book ~ ., data = _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))

weights
```

plotting importance in descending order (using reorder to order factor levels used by ggplot).

```{r}
ggplot(weights,
  aes(x = attr_importance, y = reorder(feature, attr_importance))) +
  geom_bar(stat = "identity") +
  xlab("Importance score") + 
  ylab("Feature")
```

Getting the 5 best features

```{r}
subset <- cutoff.k(weights |> 
                   column_to_rownames("feature"), 5)
subset
```
Use only the best 5 features to build a model 

```{r}
f <- as.simple.formula(subset, "book")
f
```

```{r}
m <- artists_train |> rpart(f, data = _)
rpart.plot(m, extra = 2, roundint = FALSE)
```

There are many alternative ways to calculate univariate importance scores. Some of them work for continuous features. One example is the information gain ratio based on entropy as used in decision tree induction.

```{r}
artists_train |> 
  gain.ratio(book ~ ., data = _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))
```

### Feature Subset Selection

Often features are related and calculating importance for each feature independently is not optimal. 

```{r}
artists_train |> 
  cfs(book ~ ., data = _)
```

*Black-box feature selection* uses an evaluator function (the black box) to calculate a score to be maximized. 
First, define an evaluation function that builds a model given a subset of features and calculates a quality score.
Use the average for 5 bootstrap samples (method = "cv" can also be used instead), no tuning (to be faster), and the average accuracy as the score.

```{r}
evaluator <- function(subset) {
  model <- artists_train |> 
    train(as.simple.formula(subset, "book"),
          data = _,
          method = "rpart",
          trControl = trainControl(method = "boot", number = 5),
          tuneLength = 0)
  results <- model$resample$Accuracy
  cat("Trying features:", paste(subset, collapse = " + "), "\n")
  m <- mean(results)
  cat("Accuracy:", round(m, 2), "\n\n")
  m
}
```

Starting with all features

```{r}
features <- artists_train |> colnames() |> setdiff("type")
```

### Using Dummy Variables for Factors

Nominal features (factors) are often encoded as a series of 0-1 dummy variables. 

```{r}
tree_predator <- artists_train |> 
  rpart(artist_unique_id ~ book, data = _)
rpart.plot(tree_predator, roundint = FALSE)
```

Some splits use multiple values. Building the tree will become extremely slow if a factor has many levels (different values) since the tree has to check all possible splits into two subsets. This situation should be avoided.

Converting type into a set of 0-1 dummy variables using class2ind.

```{r}
artists_train_dummy <- as_tibble(class2ind(artists_train$book)) |> 
  mutate(across(everything(), as.factor)) |>
  add_column(artist_unique_id = artists_train$artist_unique_id)
artists_train_dummy
```

```{r}
tree_predator <- artists_train_dummy |> 
  rpart(artist_unique_id ~ ., 
        data = _,
        control = rpart.control(minsplit = 2, cp = 0.01))
rpart.plot(tree_predator, roundint = FALSE)
```

Using caret on the original factor encoding automatically translates factors (here type) into 0-1 dummy variables (e.g., typeinsect = 0).

```{r}
fit <- artists_train |> 
  train(artist_unique_id ~ book, 
        data = _, 
        method = "rpart",
        control = rpart.control(minsplit = 2),
        tuneGrid = data.frame(cp = 0.01))
fit
```

```{r}
rpart.plot(fit$finalModel)
```

## Class Imbalance

Classifiers have a hard time to learn from data where we have much more observations for one class (called the majority class). This is called the class imbalance problem.

```{r}
library(rpart)
library(rpart.plot)
data(artists, package="mlbench")
```

Class Distribution

```{r}
ggplot(artists, aes(y = book)) + geom_bar()
```

To create an imbalanced problem, we should decide if a book is a Janson. First, we change the class variable to make it into a binary Janson/no Janson classification problem. 

```{r}
artists_book <- artists |> 
  mutate(type = factor(artists$book == "Janson", 
                       levels = c(FALSE, TRUE),
                       labels = c("no Janson", "Janson")))
```

```{r}
summary(artists_book)
```

checking if we have a class imbalance problem.

```{r}
ggplot(artists_book, aes(y = book)) + geom_bar()
```

Creating test and training data.

```{r}
set.seed(1234)

inTrain <- createDataPartition(y = artists_book$book, p = .5, list = FALSE)
training_book <- artists_book |> slice(inTrain)
testing_book <- artists_book |> slice(-inTrain)
```

the new class variable is clearly not balanced. This is a problem for building a tree.

### Option 1: Use the Data As Is and Hope For The Best

```{r}
fit <- training_book |> 
  train(book ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"))
```
This means that some test folds did not contain examples of both classes. This is very likely with class imbalance and small datasets.

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

The tree predicts everything as non-reptile.

```{r}
confusionMatrix(data = predict(fit, testing_book),
                ref = testing_book$book, positive = "Janson")
```

Accuracy is high, but it is exactly the same as the no-information rate and kappa is zero. 
Sensitivity is also zero, meaning that we do not identify any positive (reptile). If the cost of missing a positive is much larger than the cost associated with misclassifying a negative, then accuracy is not a good measure! 
By dealing with imbalance, we are not concerned with accuracy, but we want to increase the sensitivity, i.e., the chance to identify positive examples.

### Option 2: Balance Data With Resampling

We use stratified sampling with replacement (to oversample the minority/positive class). You could also use SMOTE (in package DMwR) or other sampling strategies 

```{r}
library(sampling)
set.seed(1000) # for repeatability

id <- strata(training_book, stratanames = "type", size = c(50, 50), method = "srswr")
training_book_balanced <- training_book |> 
  slice(id$ID_unit)
table(training_book_balanced$book)
```

```{r}
fit <- training_book_balanced |> 
  train(book ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

Checking on the unbalanced testing data.

```{r}
confusionMatrix(data = predict(fit, testing_book),
                ref = testing_book$book, positive = "Janson")
```

the accuracy is below the no information rate! However, kappa (improvement of accuracy over randomness) and sensitivity (the ability to identify reptiles) have increased.

There is a tradeoff between sensitivity and specificity (how many of the identified animals are really reptiles) The tradeoff can be controlled using the sample proportions. 

```{r}
id <- strata(training_book, stratanames = "type", size = c(50, 100), method = "srswr")
training_book_balanced <- training_book |> 
  slice(id$ID_unit)
table(training_book_balanced$book)
```

```{r}
fit <- training_book_balanced |> 
  train(book ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

confusionMatrix(data = predict(fit, testing_book),
                ref = testing_book$book, positive = "Janson")
```

### Option 3: Build A Larger Tree and use Predicted Probabilities

Increase complexity and require less data for splitting a node. 

```{r}
fit <- training_book |> 
  train(book ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv",
        classProbs = TRUE,  ## necessary for predict with type="prob"
        summaryFunction=twoClassSummary),  ## necessary for ROC
        metric = "ROC",
        control = rpart.control(minsplit = 3))
```
```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

```{r}
confusionMatrix(data = predict(fit, testing_book),
                ref = testing_book$book, positive = "Janson")
```

*Create A Biased Classifier*
We can create a classifier which will detect more reptiles at the expense of misclassifying non-reptiles. This is equivalent to increasing the cost of misclassifying a reptile as a non-reptile. The usual rule is to predict in each node the majority class from the test data in the node. For a binary classification problem that means a probability of >50%. In the following, we reduce this threshold to 1% or more. This means that if the new observation ends up in a leaf node with 1% or more reptiles from training then the observation will be classified as a reptile. The data set is small and this works better with more data.

```{r}
prob <- predict(fit, testing_book, type = "prob")
tail(prob)
```

*Plot the ROC Curve*
Since we have a binary classification problem and a classifier that predicts a probability for an observation to be a reptile, we can also use a receiver operating characteristic (ROC) curve. For the ROC curve all different cutoff thresholds for the probability are used and then connected with a line. The area under the curve represents a single number for how well the classifier works.

```{r}
library("pROC")
r <- roc(testing_book$book == "Janson", prob[,"Janson"])
```

```{r}
r
```

```{r}
ggroc(r) + geom_abline(intercept = 1, slope = 1, color = "green")
```

### Option 4: Use a Cost-Sensitive Classifier

The implementation of CART in rpart can use a cost matrix for making splitting decisions (as parameter loss). The matrix has the form

TP FP FN TN

TP and TN have to be 0.

```{r}
cost <- matrix(c(
  0,   1,
  100, 0
), byrow = TRUE, nrow = 2)
cost
```

```{r}
fit <- training_book |> 
  train(book ~ .,
        data = _,
        method = "rpart",
        parms = list(loss = cost),
        trControl = trainControl(method = "cv"))
```

This means that some folds did not contain any reptiles (because of the class imbalance) and thus the performance measures could not be calculates.

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

```{r}
confusionMatrix(data = predict(fit, testing_book),
                ref = testing_book$book, positive = "Janson")
```

The high cost for false negatives results in a classifier that does not miss any reptile.

Using a cost-sensitive classifier is often the best option. Unfortunately, the most classification algorithms (or their implementation) do not have the ability to consider misclassification cost.
